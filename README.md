### FA-LPL
- the official achievement of our proposed FA-LPL. 基于特征注意力和标签概率学习的文本分类模型

the code is based on [LCM](https://github.com/beyondguo/label_confusion_learning).

### Pre-trained Weights
- 中文： [baike_26g_news_13g_novel_229g.bin](https://pan.baidu.com/s/1ckkH_eT-WS4SN73Iq9Q_5A)(密码：9aza), 相关加载方式及链接来源在[此](https://blog.csdn.net/znsoft/article/details/107140452).
- 英文： [glove.6B.100d.txt](https://nlp.stanford.edu/projects/glove/)


### Pre_trained Model of BERT
- 中文: [albert_tiny_google_zh_489k](https://pan.baidu.com/s/1UsJRo4E8DRshwpF8rA3i9A)(密码：4m4b)
- 英文：[bert_tiny_uncased_L-2_H-128_A-2](https://storage.googleapis.com/bert_models/2020_02_20/uncased_L-2_H-128_A-2.zip)


### Datasets
- All datasets are in [there](https://drive.google.com/file/d/1Lt5UNtX-oV1p1wubT5UvLvyJH4uKHWL7/view?usp=sharing)


### Requirements

- python 3.6
- tensorflow 1.15.4
- keras 2.3.1
- bert4keras 0.10.8


### Run the demo
```
python xxx.py
```
